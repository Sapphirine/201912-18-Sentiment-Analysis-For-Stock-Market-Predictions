{"nbformat_minor": 2, "cells": [{"execution_count": 39, "cell_type": "code", "source": "from pyspark import SparkConf, SparkContext \nimport pyspark\nimport sys\nfrom collections import defaultdict\n", "outputs": [], "metadata": {}}, {"execution_count": 41, "cell_type": "code", "source": "def getData(sc, filename): \n    \"\"\"\n    Load data from raw text file into RDD and transform.\n    Hint: transfromation you will use: map(<lambda function>).\n    Args:\n        sc (SparkContext): spark context.\n        filename (string): hw2.txt cloud storage URI.\n    Returns:\n        RDD: RDD list of tuple of (<User>, [friend1, friend2, ... ]),\n        each user and a list of user's friends\n    \"\"\"\n    # read text file into RDD\n    data = sc.textFile(filename)\n    data = data.map(lambda line: line.split(\",\")).map(lambda x: (x[1],x[2])).map(lambda y: (y[0], y[1].split(' ')))\n    data = data.map(lambda x: (x[1][0], x[0]))\n    data = data.zipWithIndex().filter(lambda row_index: row_index[1] > 0).keys()\n    data = data.groupByKey()\n    #.map(lambda x,y: (x[1],[y]))\n    #dataset = data.map(lambda line: (line[0], line[1].split(',') if len (line[1]) else []))\n    # TODO: implement your logic here\n    return data\nsc = SparkContext.getOrCreate()\nfilename = 'gs://final_project123/try.csv'\ndata_new = getData(sc, filename)\n", "outputs": [], "metadata": {}}, {"execution_count": 42, "cell_type": "code", "source": "def mapFriends(line):\n    \"\"\"\n    List out every pair of mutual friends, also record direct friends.\n    Hint:\n    For each <User>, record direct friends into a list:\n    [(<User>, (friend1, 0)),(<User>, (friend2, 0)), ...],\n    where 0 means <User> and friend are already direct friend,\n    so you don't need to recommand each other.\n\n    For friends in the list, each of them has a friend <User> in common,\n    so for each of them, record mutual friend in both direction:\n    (friend1, (friend2, 1)), (friend2, (friend1, 1)),\n    where 1 means friend1 and friend2 has a mutual friend <User> in this \"line\"\n\n    There are possibly multiple output in each input line,\n    we applied flatMap to flatten them when using this function.\n    Args:\n        line (tuple): tuple in data RDD\n    Yields:\n        RDD: rdd like a list of (A, (B, 0)) or (A, (C, 1))\n    \"\"\"\n    friend = line[1]\n    user = line[0]\n    if friend == []:\n        return\n    try:\n        for i in range(len(friend)):\n        # Direct friend\n        # TODO: implement your logic here\n            yield (int(user),(int(friend[i]),0))\n        \n            for j in range(i+1, len(friend)):\n            # Mutual friend in both direction\n            # TODO: implement your logic here\n                yield (int(friend[i]),(int(friend[j]),1))\n                yield (int(friend[j]),(int(friend[i]),1))\n            \n            \n    except:\n        pass\n    ", "outputs": [], "metadata": {}}, {"execution_count": 43, "cell_type": "code", "source": "data_new.take(5)", "outputs": [{"execution_count": 43, "output_type": "execute_result", "data": {"text/plain": "[(u'swish41', <pyspark.resultiterable.ResultIterable at 0x7f1dce597890>),\n (u'God_Son80', <pyspark.resultiterable.ResultIterable at 0x7f1dce597390>),\n (u'obj', <pyspark.resultiterable.ResultIterable at 0x7f1dce597a90>),\n (u'ESPNNBApic', <pyspark.resultiterable.ResultIterable at 0x7f1dce597a50>),\n (u'NFL', <pyspark.resultiterable.ResultIterable at 0x7f1dce597f10>)]"}, "metadata": {}}], "metadata": {}}, {"execution_count": 44, "cell_type": "code", "source": "getFriends = data_new.mapValues(list)\ngetFriends.take(5)", "outputs": [{"execution_count": 44, "output_type": "execute_result", "data": {"text/plain": "[(u'swish41', [u'Ballislife', u'ESPNNBA']),\n (u'God_Son80', [u'ComplexSports', u'darrenrovell']),\n (u'obj', [u'ComplexSports', u'darrenrovell']),\n (u'ESPNNBApic', [u'Ballislife']),\n (u'NFL', [u'dailyrepubIican'])]"}, "metadata": {}}], "metadata": {}}, {"execution_count": 45, "cell_type": "code", "source": "vertices=getFriends.map(lambda x:(x[0],))", "outputs": [], "metadata": {}}, {"execution_count": 50, "cell_type": "code", "source": "vertices.collect()", "outputs": [{"execution_count": 50, "output_type": "execute_result", "data": {"text/plain": "[(u'swish41',),\n (u'God_Son80',),\n (u'obj',),\n (u'ESPNNBApic',),\n (u'NFL',),\n (u'Nike',)]"}, "metadata": {}}], "metadata": {}}, {"execution_count": 51, "cell_type": "code", "source": "def f(x): return x", "outputs": [], "metadata": {}}, {"execution_count": 52, "cell_type": "code", "source": "edges = data_new.flatMapValues(f)\n", "outputs": [], "metadata": {}}, {"execution_count": 53, "cell_type": "code", "source": "edges.collect()", "outputs": [{"execution_count": 53, "output_type": "execute_result", "data": {"text/plain": "[(u'swish41', u'Ballislife'),\n (u'swish41', u'ESPNNBA'),\n (u'God_Son80', u'ComplexSports'),\n (u'God_Son80', u'darrenrovell'),\n (u'obj', u'ComplexSports'),\n (u'obj', u'darrenrovell'),\n (u'ESPNNBApic', u'Ballislife'),\n (u'NFL', u'dailyrepubIican'),\n (u'Nike', u'ComplexSports')]"}, "metadata": {}}], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "from pyspark.sql import SparkSession\nspark = SparkSession \\\n    .builder \\\n    .appName(\"Python Spark dataframe\") \\\n    .config(\"spark.some.config.option\", \"some-value\") \\\n    .getOrCreate()", "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "v = spark.createDataFrame(vertices, [\"id\"])", "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "e = spark.createDataFrame(edges,[\"src\",\"dst\"])", "outputs": [], "metadata": {}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Python 2", "name": "python2", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "2.7.14", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython2", "codemirror_mode": {"version": 2, "name": "ipython"}}}}